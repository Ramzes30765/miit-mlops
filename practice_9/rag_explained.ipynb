{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5b3e933",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce89981",
   "metadata": {},
   "source": [
    "## 1. Что такое RAG и для чего он нужен\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** - это архитектура, в которой LLM использует не только свои параметры, но и внешние данные из хранилища (обычно векторной БД) для ответа на запрос.\n",
    "\n",
    "Основные проблемы, которые решает RAG:\n",
    "\n",
    "1. Ограничение по дате обучения модели (knowledge cutoff).\n",
    "2. Галлюцинации и вымысел.\n",
    "3. Необходимость работать с внутренними, закрытыми документами.\n",
    "\n",
    "Типичный сценарий применения:\n",
    "\n",
    "* корпоративные ассистенты;\n",
    "* поисковые системы по внутренним документам;\n",
    "* юридические/медицинские справочные системы;\n",
    "* аналитику по отчетам, регламентам, документации.\n",
    "\n",
    "## 2. Общая схема работы RAG\n",
    "\n",
    "Работу RAG обычно делят на два больших этапа:\n",
    "\n",
    "1. **Ingestion (подготовка данных)**:\n",
    "   парсинг документов --> разбиение на фрагменты (chunking) --> построение эмбеддингов --> запись во векторную БД (например, Qdrant).\n",
    "\n",
    "2. **Online-процесс (ответ на запрос)**:\n",
    "   запрос пользователя --> embedding запроса --> поиск релевантных фрагментов в Qdrant --> формирование prompt с контекстом --> генерация ответа LLM (через vLLM)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739b371e",
   "metadata": {},
   "source": [
    "## 3. Компоненты RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9721e667",
   "metadata": {},
   "source": [
    "### 3.1. Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608749ec",
   "metadata": {},
   "source": [
    "Назначение: превратить сырые документы (DOCX, PDF, HTML, TXT и т.п.) в нормализованный текст плюс полезные метаданные (название файла, раздел, номер страницы и т.д.).\n",
    "\n",
    "Ключевые задачи:\n",
    "\n",
    "* извлечь текст;\n",
    "* очистить от шума (лишние переводы строк, номера страниц, оглавления);\n",
    "* сохранить структуру (заголовки, разделы), если это важно для дальнейшего chunking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa95dcee",
   "metadata": {},
   "source": [
    "### 3.2. Chunker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a6150a",
   "metadata": {},
   "source": [
    "**Задача чанкинга** - разбить длинный текст на фрагменты (chunks), которые:\n",
    "\n",
    "1. Достаточно короткие, чтобы помещаться в контекст LLM.\n",
    "2. Достаточно информативные, чтобы каждый chunk содержал законченную мысль.\n",
    "\n",
    "Типичные подходы:\n",
    "\n",
    "1. **Фиксированный размер по токенам/символам**\n",
    "   Простой вариант: брать, например, 500 токенов с overlap 100 токенов.\n",
    "   Плюсы: простота реализации.\n",
    "   Минусы: разрыв логики (предложение может резаться посередине), слабая семантика.\n",
    "\n",
    "2. **Чанк по структуре документа**\n",
    "   Разбиение по заголовкам, абзацам, спискам.\n",
    "   Плюсы: chunks ближе к логическим блокам документа.\n",
    "   Минусы: разный размер фрагментов, иногда очень длинные разделы.\n",
    "\n",
    "3. **Semantic chunking**\n",
    "   Используются эвристики или модели (например, кластеризация предложений) для объединения предложений в семантически однородные блоки.\n",
    "   Плюсы: лучшие результаты при поиске.\n",
    "   Минусы: сложнее реализация, больше вычислений."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82ba281",
   "metadata": {},
   "source": [
    "### 3.3. Embedder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a9df05",
   "metadata": {},
   "source": [
    "**Embedder** - модель, которая переводит текст в векторы фиксированной размерности (эмбеддинги).\n",
    "\n",
    "Назначение:\n",
    "\n",
    "* Для каждого chunk вычислить embedding и записать его в векторную БД.\n",
    "* Для каждого пользовательского запроса - также получить embedding и использовать его для поиска.\n",
    "\n",
    "Типы моделей:\n",
    "\n",
    "* Специализированные embedding-модели: BGE-M3, E5, Instructor, GTE и т.д.\n",
    "* Embedding-эндпоинты облачных провайдеров (OpenAI, Azure, и пр.).\n",
    "\n",
    "Ключевые характеристики эмбеддера:\n",
    "\n",
    "* **Размерность вектора** (например, 768, 1024, 1536). Она должна совпадать с размерностью коллекции в Qdrant.\n",
    "* **Тип задачи**:\n",
    "\n",
    "- **text embedding** (для query + passage)\n",
    "- **multi‑lingual embedding**\n",
    "- **cross‑encoder vs bi‑encoder** (для reranking)\n",
    "\n",
    "Рабочий цикл:\n",
    "\n",
    "1. На этапе ingestion:\n",
    "   chunk --> embedder --> вектор --> запись в Qdrant.\n",
    "2. На этапе ответа:\n",
    "   query --> embedder --> вектор --> поиск в Qdrant --> контекст."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6589836",
   "metadata": {},
   "source": [
    "## 4. Qdrant: векторная БД\n",
    "\n",
    "Qdrant - это векторная база данных, которая хранит векторы и метаданные и предоставляет API для:\n",
    "\n",
    "* создания и управления коллекциями;\n",
    "* записи и обновления точек (points);\n",
    "* поиска ближайших векторов;\n",
    "* фильтрации по метаданным;\n",
    "* шардирования и репликации.\n",
    "\n",
    "### 4.1. Базовые сущности\n",
    "\n",
    "* **Collection** - логическое хранилище векторов и метаданных.\n",
    "* **Point** - объект с полями:\n",
    "\n",
    "  * `id` - уникальный идентификатор;\n",
    "  * `vector` - embedding;\n",
    "  * `payload` - произвольные метаданные (текст chunk-а, source, тип документа и т.д.).\n",
    "\n",
    "\n",
    "### 4.2. Основные методы Qdrant\n",
    "\n",
    "С точки зрения RAG-important операций:\n",
    "\n",
    "1. **Создание коллекции**\n",
    "\n",
    "   * `create_collection`\n",
    "     Определяется:\n",
    "\n",
    "     * размерность вектора (`size`);\n",
    "     * метрика (`distance` - cosine, dot, euclid);\n",
    "     * конфигурация HNSW, sharding, количество реплик.\n",
    "\n",
    "2. **Информация о коллекции**\n",
    "\n",
    "   * `get_collection_info`\n",
    "     Возвращает конфигурацию и состояние коллекции (размер, количество точек, конфиг индекса и т.д.).\n",
    "\n",
    "3. **Запись данных**\n",
    "\n",
    "   * `upsert`\n",
    "     Добавление или обновление точек.\n",
    "     Используется в ingestion-пайплайне.\n",
    "     Принимает список `PointStruct` c `id`, `vector`, `payload`.\n",
    "\n",
    "   * `delete`\n",
    "     Удаление точек по id или по фильтру.\n",
    "\n",
    "4. **Поиск**\n",
    "\n",
    "   * `search`\n",
    "     Классический nearest-neighbor поиск по одному вектору (query_vector).\n",
    "     Параметры:\n",
    "\n",
    "     * `collection_name`\n",
    "     * `query_vector`\n",
    "     * `limit` (top-K)\n",
    "     * `filter` (по payload: например, по типу документа, дате, языку)\n",
    "     * `with_payload`, `with_vectors`\n",
    "\n",
    "   * `query_points` (в новых версиях)\n",
    "     Расширенный запрос, который поддерживает:\n",
    "\n",
    "     * поиск по нескольким векторным полям;\n",
    "     * скоры;\n",
    "     * дополнительные настройки.\n",
    "\n",
    "   * `scroll`\n",
    "     «Прокрутка» по коллекции: постраничный проход через все `points` (для отладки, миграций, бэкапов).\n",
    "\n",
    "5. **Управление индексами**\n",
    "\n",
    "   * `create_field_index`, `delete_field_index`\n",
    "     Индексы по payload-полям для ускорения фильтрации.\n",
    "\n",
    "6. **Обновление конфигурации**\n",
    "\n",
    "   * `update_collection`, `update_cluster` и др.\n",
    "     Настройки шардирования, репликации, оптимизации.\n",
    "\n",
    "\n",
    "### 4.3. Режимы и типы поиска в Qdrant\n",
    "\n",
    "1. **Vector similarity search**\n",
    "\n",
    "   * Поиск ближайших векторов по метрике (cosine / dot / euclid).\n",
    "     Плюсы: работает с высоким семантическим сходством;\n",
    "     Минусы: иногда возвращает семантически схожие, но не релевантные конкретному вопросу chunks.\n",
    "\n",
    "2. **Hybrid search (vector + keyword)**\n",
    "\n",
    "   Можно комбинировать:\n",
    "\n",
    "   * Vector search (по embedding)\n",
    "   * Keyword/BM25 (по тексту или по payload)\n",
    "\n",
    "   Обычно реализуется как последовательное или комбинированное:\n",
    "\n",
    "   * сначала BM25 фильтрует кандидатов --> затем vector reranking;\n",
    "   * либо vector search + фильтр по полям.\n",
    "\n",
    "   Плюсы: лучше работает для специфических терминов, кодов, чисел;\n",
    "   Минусы: сложнее организовать, нужна дополнительная инфраструктура (например, отдельный text index).\n",
    "\n",
    "3. **Filtered search**\n",
    "\n",
    "   В `filter` можно указать условия по payload, например:\n",
    "\n",
    "   * тип документа;\n",
    "   * дата;\n",
    "   * язык;\n",
    "   * определенные теги.\n",
    "\n",
    "   Плюсы: позволяет сузить поиск до пригодного подмножества данных (например, только документы определенного проекта).\n",
    "   Минусы: требует грамотного проектирования payload-структуры.\n",
    "\n",
    "4. **Multi-vector / multi-field search**\n",
    "\n",
    "   Qdrant поддерживает коллекции с несколькими векторными полями (например, `content_vector`, `title_vector`).\n",
    "   Поиск может учитывать оба поля, с разными весами.\n",
    "\n",
    "   Плюсы: можно сочетать разные типы эмбеддингов;\n",
    "   Минусы: сложнее настройка и балансировка."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b4fcd7",
   "metadata": {},
   "source": [
    "## 5. vLLM: сервер инференса LLM\n",
    "\n",
    "vLLM - это high-performance сервер инференса LLM с упором на:\n",
    "\n",
    "* эффективное управление KV-кэшем (PagedAttention);\n",
    "* высокую пропускную способность по токенам;\n",
    "* OpenAI-совместимый API.\n",
    "\n",
    "Используется как backend для:\n",
    "\n",
    "* генерации ответов RAG;\n",
    "* reranking;\n",
    "* query rewriting;\n",
    "* summarization.\n",
    "\n",
    "### 5.1. Запуск vLLM\n",
    "\n",
    "Базовый запуск:\n",
    "\n",
    "```bash\n",
    "vllm serve your-model-name \\\n",
    "  --port 8000 \\\n",
    "  --host 0.0.0.0\n",
    "```\n",
    "\n",
    "```bash\n",
    "vllm  | INFO 12-04 08:26:12 [api_server.py:1090] Starting vLLM API server on http://0.0.0.0:8000\n",
    "vllm  | INFO 12-04 08:26:12 [launcher.py:28] Available routes are:\n",
    "vllm  | INFO 12-04 08:26:12 [launcher.py:36] Route: /openapi.json, Methods: HEAD, GET\n",
    "vllm  | INFO 12-04 08:26:12 [launcher.py:36] Route: /docs, Methods: HEAD, GET\n",
    "vllm  | INFO 12-04 08:26:12 [launcher.py:36] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
    "vllm  | INFO 12-04 08:26:12 [launcher.py:36] Route: /redoc, Methods: HEAD, GET\n",
    "vllm  | INFO 12-04 08:26:12 [launcher.py:36] Route: /health, Methods: GET\n",
    "vllm  | INFO 12-04 08:26:12 [launcher.py:36] Route: /load, Methods: GET\n",
    "vllm  | INFO 12-04 08:26:12 [launcher.py:36] Route: /ping, Methods: GET, POST\n",
    "vllm  | INFO 12-04 08:26:12 [launcher.py:36] Route: /tokenize, Methods: POST\n",
    "vllm  | INFO 12-04 08:26:12 [launcher.py:36] Route: /detokenize, Methods: POST\n",
    "vllm  | INFO 12-04 08:26:12 [launcher.py:36] Route: /v1/models, Methods: GET\n",
    "vllm  | INFO 12-04 08:26:12 [launcher.py:36] Route: /version, Methods: GET\n",
    "vllm  | INFO 12-04 08:26:12 [launcher.py:36] Route: /v1/chat/completions, Methods: POST\n",
    "vllm  | INFO 12-04 08:26:12 [launcher.py:36] Route: /v1/completions, Methods: POST\n",
    "vllm  | INFO 12-04 08:26:12 [launcher.py:36] Route: /v1/embeddings, Methods: POST\n",
    "vllm  | INFO 12-04 08:26:12 [launcher.py:36] Route: /pooling, Methods: POST\n",
    "vllm  | INFO 12-04 08:26:12 [launcher.py:36] Route: /score, Methods: POST\n",
    "vllm  | INFO 12-04 08:26:12 [launcher.py:36] Route: /v1/score, Methods: POST\n",
    "vllm  | INFO 12-04 08:26:12 [launcher.py:36] Route: /v1/audio/transcriptions, Methods: POST\n",
    "vllm  | INFO 12-04 08:26:12 [launcher.py:36] Route: /rerank, Methods: POST\n",
    "vllm  | INFO 12-04 08:26:12 [launcher.py:36] Route: /v1/rerank, Methods: POST\n",
    "vllm  | INFO 12-04 08:26:12 [launcher.py:36] Route: /v2/rerank, Methods: POST\n",
    "vllm  | INFO 12-04 08:26:12 [launcher.py:36] Route: /invocations, Methods: POST\n",
    "vllm  | INFO 12-04 08:26:12 [launcher.py:36] Route: /metrics, Methods: GET\n",
    "vllm  | INFO:     Started server process [1]\n",
    "vllm  | INFO:     Waiting for application startup.\n",
    "vllm  | INFO:     Application startup complete.\n",
    "vllm  | INFO:     172.19.0.1:36838 - \"GET /health HTTP/1.1\" 200 OK\n",
    "```\n",
    "\n",
    "\n",
    "### 5.2. Важные флаги vLLM\n",
    "\n",
    "1. **Модель и пути**\n",
    "\n",
    "   * `--model`\n",
    "     Название модели из HuggingFace или локальный путь.\n",
    "\n",
    "   * `--download-dir`\n",
    "     Каталог, куда качать веса. Позволяет кешировать модели между перезапусками.\n",
    "\n",
    "2. **Аппаратные ресурсы**\n",
    "\n",
    "   * `--tensor-parallel-size`\n",
    "     Количество GPU, используемых для tensor parallelism.\n",
    "\n",
    "   * `--dtype`\n",
    "     Тип чисел: `float16`, `bfloat16`, иногда `float32`.\n",
    "     Важен баланс между точностью и памятью.\n",
    "\n",
    "   * `--max-num-batched-tokens`\n",
    "     Ограничение на общее число токенов в батче (сумма по всем запросам).\n",
    "     Влияет на throughput и стабильность.\n",
    "\n",
    "   * `--max-num-seqs`\n",
    "     Максимальное число последовательностей (запросов) в батче.\n",
    "\n",
    "   * `--gpu-memory-utilization`\n",
    "     Целевой процент использования GPU памяти (например, 0.9). vLLM подгоняет под это размер кэша и другие параметры.\n",
    "\n",
    "3. **Контекст и кэш**\n",
    "\n",
    "   * `--max-context-len` или `--max-model-len`\n",
    "     Максимальная длина контекста (prompt + generated tokens).\n",
    "     Чем больше, тем выше требования к памяти.\n",
    "\n",
    "   * Параметры, связанные с PagedAttention и кэшированием KV (в разных версиях - разные флаги), определяют, сколько токенов истории можно держать и как эффективно переиспользовать их между запросами.\n",
    "\n",
    "4. **Сервер и API**\n",
    "\n",
    "   * `--port`\n",
    "\n",
    "   * `--host`\n",
    "\n",
    "   * `--served-model-name`\n",
    "     Удобно, если нужно публиковать модель под другим именем, чем на HF.\n",
    "\n",
    "   * `--enable-metrics`\n",
    "     Экспорт метрик (обычно в формате Prometheus).\n",
    "\n",
    "   * `--log-level`\n",
    "     Для отладки и мониторинга.\n",
    "\n",
    "5. **Batching и планировщик**\n",
    "\n",
    "   vLLM автоматически выполняет smart batching запросов.\n",
    "   Но параметры `--max-num-batched-tokens`, `--max-num-seqs` и возможные флаги, связанные с временем ожидания batch (например, max batch latency), сильно влияют на latency vs throughput.\n",
    "\n",
    "   Настройки аналогичны Triton:\n",
    "\n",
    "   * При высоком потоке запросов есть смысл позволить больший batch, чтобы повысить throughput.\n",
    "   * Для интерактивных сценариев иногда нужно уменьшать batch или ограничивать задержку формирования batch.\n",
    "\n",
    "\n",
    "### 5.3. Использование vLLM как OpenAI-совместимого API\n",
    "\n",
    "После запуска можно использовать такой код:\n",
    "\n",
    "```python\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\",\n",
    "    api_key=\"dummy\",\n",
    ")\n",
    "\n",
    "resp = client.chat.completions.create(\n",
    "    model=\"your-model-name\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Ты помощник.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Объясни, что такое RAG.\"},\n",
    "    ],\n",
    ")\n",
    "```\n",
    "\n",
    "Это удобно, потому что:\n",
    "\n",
    "* не нужно менять клиентский код при переходе от внешнего OpenAI к локальному vLLM;\n",
    "* можно использовать те же структуры запросов (chat/completions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c74f00",
   "metadata": {},
   "source": [
    "## 6. Способы улучшить базовый RAG\n",
    "\n",
    "Базовая схема: query --> embedder --> Qdrant --> контекст --> LLM.\n",
    "Эту схему можно существенно улучшить за счет дополнительных компонент.\n",
    "\n",
    "### 6.1. Query Rewriter\n",
    "\n",
    "Модуль, который переписывает запрос перед retrieval:\n",
    "\n",
    "* убирает лишние детали и шум;\n",
    "* добавляет недостающий контекст;\n",
    "* разбивает сложный вопрос на несколько подзапросов;\n",
    "* переводит на язык документов (например, формализует запрос под официальную терминологию).\n",
    "\n",
    "Реализация:\n",
    "\n",
    "* отдельная LLM (через vLLM) с промптом вида:\n",
    "  «Перепиши запрос пользователя так, чтобы он лучше подходил для поиска по базе документов…».\n",
    "\n",
    "Эффект: увеличивается recall релевантных документов.\n",
    "\n",
    "\n",
    "### 6.2. Reranker (Cross-Encoder / LLM-Reranker)\n",
    "\n",
    "После начального поиска по эмбеддингам можно прогнать top-N документов через reranker:\n",
    "\n",
    "1. На первом этапе Qdrant возвращает, например, 20–50 кандидатов по cosine similarity.\n",
    "2. Reranker оценивает парой (query, chunk) и выдает более точный «релевантность score».\n",
    "3. Берутся top-K после reranking (например, 5–10).\n",
    "\n",
    "Варианты:\n",
    "\n",
    "* готовые модели reranking (например, cross-encoder с HuggingFace);\n",
    "* LLM, который действует как reranker: на вход подается query и несколько кандидатов, на выход - ранжированный список.\n",
    "\n",
    "Плюсы:\n",
    "\n",
    "* значительно снижает число нерелевантных chunk-ов в контексте.\n",
    "\n",
    "Минусы:\n",
    "\n",
    "* дополнительная нагрузка (дополнительные вызовы модели).\n",
    "\n",
    "\n",
    "### 6.3. Multi-step / Decomposed RAG\n",
    "\n",
    "Используется для сложных запросов:\n",
    "\n",
    "1. LLM анализирует вопрос и разлагает его на подзадачи.\n",
    "2. Для каждой подзадачи выполняется retrieval и частичное решение.\n",
    "3. Финальный ответ собирается из нескольких шагов reasoning.\n",
    "\n",
    "Примеры техник:\n",
    "\n",
    "* Chain-of-Thought RAG;\n",
    "* Tree-of-Thought RAG;\n",
    "* Tool-augmented RAG (LLM как оркестратор инструментов).\n",
    "\n",
    "\n",
    "### 6.4. Context Optimization\n",
    "\n",
    "Несколько техник:\n",
    "\n",
    "* **Контекстное сжатие (context compression)** - длинные найденные chunk-и сначала суммируются/сжимаются, затем подаются в LLM.\n",
    "* **Adaptive context window** - динамический выбор количества chunk-ов в зависимости от сложности вопроса.\n",
    "* **Metadata-aware retrieval** - использование фильтров по метаданным для уменьшения шума.\n",
    "\n",
    "\n",
    "### 6.5. Knowledge Graph / GraphRAG\n",
    "\n",
    "При наличии сложной структуры знаний:\n",
    "\n",
    "* выделяются сущности и связи;\n",
    "* строится граф (knowledge graph);\n",
    "* при запросе выполняется traversal по графу и выбор релевантных узлов;\n",
    "* затем результаты передаются в LLM.\n",
    "\n",
    "Плюсы: хорошо для аналитических запросов, когда важна структура и причинно-следственные связи.\n",
    "Минусы: сложность обработки и поддержки."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
