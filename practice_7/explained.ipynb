{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3d97db4",
   "metadata": {},
   "source": [
    "# Оптимизация графа вычислений на примере слияния Conv–BatchNorm–ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae33d7c",
   "metadata": {},
   "source": [
    "## 1. Введение\n",
    "\n",
    "Во время оптимизации графа вычислений фреймворки, такие как ONNX Runtime, TensorRT или OpenVINO, объединяют последовательные операции свёртки (Conv), нормализации (BatchNorm) и активации (ReLU) в одну эквивалентную, но более эффективную операцию.  \n",
    "\n",
    "Основная причина этого — уменьшение количества обращений к памяти и запусков отдельных вычислительных ядер на GPU или CPU.  \n",
    "Главное математическое основание: и свёртка, и нормализация представляют собой **линейные преобразования**, которые могут быть объединены в одно эквивалентное линейное преобразование."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d357a4af",
   "metadata": {},
   "source": [
    "## 2. Свёрточная операция (Convolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9972a3ed",
   "metadata": {},
   "source": [
    "Свёртка в свёрточной нейронной сети — это операция линейного преобразования, вычисляемая по формуле\n",
    "\n",
    "$$\n",
    "Y[i,j] = \\sum_{u=0}^{H_W-1} \\sum_{v=0}^{W_W-1} W[u,v] \\cdot X[i+u, j+v] + b\n",
    "$$\n",
    "\n",
    "где  \n",
    "-  X  — входное изображение,  \n",
    "-  W  — фильтр (ядро свёртки),  \n",
    "-  b  — смещение (bias),  \n",
    "-  Y  — выход свёртки,  \n",
    "-  H_W, W_W  — высота и ширина фильтра."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899c1724",
   "metadata": {},
   "source": [
    "### 2.1. Параметры свёртки\n",
    "\n",
    "- **Stride (шаг свёртки)** — насколько ядро сдвигается по входу.  \n",
    "- **Padding** — добавление рамки нулей вокруг входа.  \n",
    "- **Valid padding** — свёртка без добавления нулей (padding = 0).  \n",
    "- **Same padding** — паддинг подбирается так, чтобы выход имел те же размеры, что и вход.\n",
    "\n",
    "Для valid-свёртки (padding = 0) размер выхода равен:\n",
    "\n",
    "$$\n",
    "H_{out} = H_{in} - H_W + 1, \\quad\n",
    "W_{out} = W_{in} - W_W + 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea78763",
   "metadata": {},
   "source": [
    "## 3. Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ca2c73",
   "metadata": {},
   "source": [
    "Batch Normalization в режиме обучения нормализует активации внутри батча, а затем масштабирует и сдвигает их с помощью обучаемых параметров $ \\gamma $ и $ \\beta $:\n",
    "\n",
    "$$\n",
    "\\text{BN}(y) = \\gamma \\frac{y - \\mu_{\\text{batch}}}{\\sqrt{\\sigma^2_{\\text{batch}} + \\epsilon}} + \\beta\n",
    "$$\n",
    "\n",
    "- $ \\mu_{\\text{batch}} $ — среднее по батчу,  \n",
    "- $ \\sigma^2_{\\text{batch}} $ — дисперсия по батчу,  \n",
    "- $ \\epsilon $ — стабилизирующая константа,  \n",
    "- $ \\gamma, \\beta $ — обучаемые параметры."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69b6256",
   "metadata": {},
   "source": [
    "### 3.1. BatchNorm на inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d755510",
   "metadata": {},
   "source": [
    "На этапе inference используются накопленные статистики (running mean и running variance):\n",
    "\n",
    "$$\n",
    "\\text{BN}(y) = \\gamma \\frac{y - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta\n",
    "$$\n",
    "\n",
    "Это линейная операция над каждым элементом $ y $, которую можно записать в виде\n",
    "\n",
    "$$\n",
    "\\text{BN}(y) = a y + c,\n",
    "$$\n",
    "\n",
    "где\n",
    "\n",
    "$$\n",
    "a = \\frac{\\gamma}{\\sqrt{\\sigma^2 + \\epsilon}}, \\quad c = \\beta - a \\mu.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc1f47c",
   "metadata": {},
   "source": [
    "#### Преобразование формулы Batch Normalization\n",
    "\n",
    "Рассмотрим пошаговое преобразование формулы Batch Normalization в режиме inference, чтобы показать, как она сводится к линейной форме.\n",
    "\n",
    "---\n",
    "\n",
    "##### Шаг 1. Определение BatchNorm в режиме inference\n",
    "\n",
    "В режиме вывода (inference) Batch Normalization вычисляется по формуле:\n",
    "\n",
    "$$\n",
    "\\text{BN}(y) = \\gamma \\frac{y - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta\n",
    "$$\n",
    "\n",
    "где  \n",
    "- $\\gamma$ и $\\beta$ — обучаемые параметры (масштаб и смещение),  \n",
    "- $\\mu$ и $\\sigma^2$ — накопленные оценки среднего и дисперсии,  \n",
    "- $\\epsilon$ — малая добавка для численной стабильности.\n",
    "\n",
    "---\n",
    "\n",
    "##### Шаг 2. Вынесение константы перед скобкой\n",
    "\n",
    "Выражение под дробью можно переписать как:\n",
    "\n",
    "$$\n",
    "\\frac{y - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} =\n",
    "\\frac{1}{\\sqrt{\\sigma^2 + \\epsilon}} \\cdot (y - \\mu)\n",
    "$$\n",
    "\n",
    "Тем самым знаменатель $\\sqrt{\\sigma^2 + \\epsilon}$ выносится за скобку как константа.\n",
    "\n",
    "---\n",
    "\n",
    "##### Шаг 3. Введение коэффициента масштабирования\n",
    "\n",
    "Теперь умножим на параметр $\\gamma$:\n",
    "\n",
    "$$\n",
    "\\gamma \\cdot \\frac{1}{\\sqrt{\\sigma^2 + \\epsilon}}\n",
    "= \\frac{\\gamma}{\\sqrt{\\sigma^2 + \\epsilon}} = a\n",
    "$$\n",
    "\n",
    "Коэффициент $a$ представляет собой масштаб, с которым нормализованные значения $y$ будут растянуты.  \n",
    "Например, при $\\gamma = 3$ и $\\sigma^2 = 4$:\n",
    "\n",
    "$$\n",
    "a = \\frac{3}{\\sqrt{4}} = \\frac{3}{2} = 1.5\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "##### Шаг 4. Раскрытие скобок\n",
    "\n",
    "Подставим определение $a$ в формулу:\n",
    "\n",
    "$$\n",
    "\\gamma \\frac{y - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} = a(y - \\mu)\n",
    "$$\n",
    "\n",
    "Раскрывая скобки, получаем:\n",
    "\n",
    "$$\n",
    "a(y - \\mu) = a y - a \\mu\n",
    "$$\n",
    "\n",
    "Это простое алгебраическое разложение, показывающее, что нормализация представляет собой линейную комбинацию исходного $y$.\n",
    "\n",
    "---\n",
    "\n",
    "##### Шаг 5. Добавление параметра смещения\n",
    "\n",
    "После раскрытия скобок добавляем $\\beta$:\n",
    "\n",
    "$$\n",
    "\\text{BN}(y) = a(y - \\mu) + \\beta\n",
    "$$\n",
    "\n",
    "Раскроем выражение полностью:\n",
    "\n",
    "$$\n",
    "\\text{BN}(y) = a y - a \\mu + \\beta\n",
    "$$\n",
    "\n",
    "Сгруппируем слагаемые по смыслу:\n",
    "\n",
    "- часть $a y$ отвечает за масштабирование,  \n",
    "- часть $(\\beta - a \\mu)$ отвечает за смещение.\n",
    "\n",
    "---\n",
    "\n",
    "### Итоговая форма\n",
    "\n",
    "Итоговая формула Batch Normalization в линейной форме:\n",
    "\n",
    "$$\n",
    "\\text{BN}(y) = a y + (\\beta - a \\mu)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19baa4fa",
   "metadata": {},
   "source": [
    "## 4. Слияние Conv и BatchNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0638634b",
   "metadata": {},
   "source": [
    "### Числовой пример"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463d0150",
   "metadata": {},
   "source": [
    "### Шаг 1. Свёртка $Y = W * X + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8a005e",
   "metadata": {},
   "source": [
    "#### Исходные данные\n",
    "\n",
    "- Входное изображение $X \\in \\mathbb{R}^{3 \\times 3}$:\n",
    "\n",
    "$$\n",
    "X =\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 0 \\\\\n",
    "0 & 1 & 3 \\\\\n",
    "2 & 2 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- Фильтр (ядро свёртки) $W \\in \\mathbb{R}^{2 \\times 2}$:\n",
    "\n",
    "$$\n",
    "W =\n",
    "\\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "-1 & 2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- Смещение (bias): $b = 0$\n",
    "\n",
    "- Параметры свёртки:\n",
    "  - шаг (stride) = 1  \n",
    "  - заполнение (padding) = 0  \n",
    "  - режим — **valid**, то есть без добавления нулей.\n",
    "\n",
    "---\n",
    "\n",
    "#### Размер выхода\n",
    "\n",
    "Для свёртки с valid-паддингом выходной размер вычисляется по формулам:\n",
    "\n",
    "$$\n",
    "H_{out} = H_{in} - H_W + 1, \\quad\n",
    "W_{out} = W_{in} - W_W + 1\n",
    "$$\n",
    "\n",
    "Подставляя значения $H_{in} = 3$, $H_W = 2$:\n",
    "\n",
    "$$\n",
    "H_{out} = 3 - 2 + 1 = 2, \\quad W_{out} = 2\n",
    "$$\n",
    "\n",
    "Следовательно, результат свёртки $Y \\in \\mathbb{R}^{2 \\times 2}$.\n",
    "\n",
    "---\n",
    "\n",
    "#### Формула элемента выхода\n",
    "\n",
    "Каждый элемент $Y[i, j]$ вычисляется как:\n",
    "\n",
    "$$\n",
    "Y[i,j] = \\sum_{u=0}^{1}\\sum_{v=0}^{1} W[u,v] \\cdot X[i+u, j+v] + b\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Пошаговые расчёты элементов выхода\n",
    "\n",
    "1. **Элемент $Y[0,0]$**  \n",
    "   Окно входа $X[0:2, 0:2]$:\n",
    "\n",
    "   $$\n",
    "   \\begin{bmatrix}\n",
    "   1 & 2 \\\\\n",
    "   0 & 1\n",
    "   \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   Тогда\n",
    "\n",
    "   $$\n",
    "   Y[0,0] = 1 \\cdot 1 + 0 \\cdot 2 + (-1) \\cdot 0 + 2 \\cdot 1 + 0 = 3\n",
    "   $$\n",
    "\n",
    "2. **Элемент $Y[0,1]$**  \n",
    "   Окно входа $X[0:2, 1:3]$:\n",
    "\n",
    "   $$\n",
    "   \\begin{bmatrix}\n",
    "   2 & 0 \\\\\n",
    "   1 & 3\n",
    "   \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "   Y[0,1] = 1 \\cdot 2 + 0 \\cdot 0 + (-1) \\cdot 1 + 2 \\cdot 3 = 7\n",
    "   $$\n",
    "\n",
    "3. **Элемент $Y[1,0]$**  \n",
    "   Окно входа $X[1:3, 0:2]$:\n",
    "\n",
    "   $$\n",
    "   \\begin{bmatrix}\n",
    "   0 & 1 \\\\\n",
    "   2 & 2\n",
    "   \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "   Y[1,0] = 1 \\cdot 0 + 0 \\cdot 1 + (-1) \\cdot 2 + 2 \\cdot 2 = 2\n",
    "   $$\n",
    "\n",
    "4. **Элемент $Y[1,1]$**  \n",
    "   Окно входа $X[1:3, 1:3]$:\n",
    "\n",
    "   $$\n",
    "   \\begin{bmatrix}\n",
    "   1 & 3 \\\\\n",
    "   2 & 1\n",
    "   \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "   Y[1,1] = 1 \\cdot 1 + 0 \\cdot 3 + (-1) \\cdot 2 + 2 \\cdot 1 = 1\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "#### Итог результата свёртки\n",
    "\n",
    "$$\n",
    "Y =\n",
    "\\begin{bmatrix}\n",
    "3 & 7 \\\\\n",
    "2 & 1\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84babc7b",
   "metadata": {},
   "source": [
    "### Шаг 2. Batch Normalization на этапе inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7cab55",
   "metadata": {},
   "source": [
    "#### Формула Batch Normalization\n",
    "\n",
    "В режиме inference Batch Normalization вычисляется по формуле:\n",
    "\n",
    "$$\n",
    "\\text{BN}(y) = \\gamma \\frac{y - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta\n",
    "$$\n",
    "\n",
    "где  \n",
    "- $\\gamma$ — обучаемый параметр масштабирования,  \n",
    "- $\\beta$ — обучаемый параметр смещения,  \n",
    "- $\\mu$ — среднее значение, накопленное за время обучения,  \n",
    "- $\\sigma^2$ — накопленная дисперсия,  \n",
    "- $\\epsilon$ — малая добавка для предотвращения деления на ноль.\n",
    "\n",
    "Эта формула представляет собой **линейное преобразование** выходных значений свёртки.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3281e0",
   "metadata": {},
   "source": [
    "### Шаг 3. Слияние Batch Normalization со свёрткой"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bfdb2c",
   "metadata": {},
   "source": [
    "На предыдущем шаге было показано, что Batch Normalization при inference сводится к линейному преобразованию вида:\n",
    "\n",
    "$$\n",
    "\\text{BN}(y) = a y + (\\beta - a \\mu),\n",
    "$$\n",
    "\n",
    "где  \n",
    "$a = \\frac{\\gamma}{\\sqrt{\\sigma^2 + \\epsilon}}$ — коэффициент масштабирования,  \n",
    "$\\beta - a\\mu$ — коэффициент смещения.\n",
    "\n",
    "Теперь подставим в эту формулу результат свёртки $Y = W * X + b$.\n",
    "\n",
    "---\n",
    "\n",
    "#### Подстановка выражения свёртки\n",
    "\n",
    "$$\n",
    "\\text{BN}(W * X + b)\n",
    "= a(W * X + b) + (\\beta - a\\mu)\n",
    "$$\n",
    "\n",
    "Раскроем скобки:\n",
    "\n",
    "$$\n",
    "= a(W * X) + a b + (\\beta - a\\mu)\n",
    "$$\n",
    "\n",
    "Сгруппируем члены, зависящие от входа $X$, и постоянные члены отдельно:\n",
    "\n",
    "$$\n",
    "= (a W) * X + (a b + \\beta - a\\mu)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Определение новых параметров свёртки\n",
    "\n",
    "Сравнивая это выражение с формулой стандартной свёртки\n",
    "\n",
    "$$\n",
    "Y' = W' * X + b',\n",
    "$$\n",
    "\n",
    "можно определить новые параметры:\n",
    "\n",
    "$$\n",
    "W' = a W, \\quad b' = a b + \\beta - a\\mu.\n",
    "$$\n",
    "\n",
    "Таким образом, последовательное применение операций **Conv → BatchNorm** эквивалентно одной свёртке с новыми параметрами $W'$ и $b'$.\n",
    "\n",
    "---\n",
    "\n",
    "#### Интерпретация\n",
    "\n",
    "- $W'$ — новая матрица фильтров, полученная масштабированием исходных весов на коэффициент $a$.  \n",
    "- $b'$ — новое смещение, учитывающее как исходный bias, так и влияние параметров нормализации $\\gamma$, $\\beta$, $\\mu$ и $\\sigma^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91982e50",
   "metadata": {},
   "source": [
    "#### Подставляем параметры примера\n",
    "\n",
    "Используем данные, полученные на предыдущем шаге:\n",
    "\n",
    "- $\\gamma = 3$\n",
    "- $\\beta = -1$\n",
    "- $\\mu = 2$\n",
    "- $\\sigma^2 = 4$\n",
    "- $\\epsilon = 0$\n",
    "\n",
    "Результат свёртки из Шага 1:\n",
    "\n",
    "$$\n",
    "Y =\n",
    "\\begin{bmatrix}\n",
    "3 & 7 \\\\\n",
    "2 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Расчёт коэффициентов нормализации\n",
    "\n",
    "Вычисляем знаменатель:\n",
    "\n",
    "$$\n",
    "\\sqrt{\\sigma^2 + \\epsilon} = \\sqrt{4} = 2\n",
    "$$\n",
    "\n",
    "Коэффициент масштабирования:\n",
    "\n",
    "$$\n",
    "a = \\frac{\\gamma}{\\sqrt{\\sigma^2 + \\epsilon}} = \\frac{3}{2} = 1.5\n",
    "$$\n",
    "\n",
    "Теперь формула BatchNorm принимает вид:\n",
    "\n",
    "$$\n",
    "\\text{BN}(y) = 1.5 \\cdot y + (\\beta - a \\mu)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Расчёт смещения\n",
    "\n",
    "Подставим известные значения:\n",
    "\n",
    "$$\n",
    "\\beta - a \\mu = -1 - (1.5 \\cdot 2) = -1 - 3 = -4\n",
    "$$\n",
    "\n",
    "Тогда итоговая формула для BatchNorm:\n",
    "\n",
    "$$\n",
    "\\text{BN}(y) = 1.5 \\cdot y - 4\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Применение BatchNorm покомпонентно\n",
    "\n",
    "Каждый элемент матрицы $Y$ масштабируется и сдвигается по формуле выше:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "1.5 \\cdot 3 - 4 &= 0.5 \\\\\n",
    "1.5 \\cdot 7 - 4 &= 6.5 \\\\\n",
    "1.5 \\cdot 2 - 4 &= -1.0 \\\\\n",
    "1.5 \\cdot 1 - 4 &= -2.5\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Получаем результат:\n",
    "\n",
    "$$\n",
    "\\text{BN}(Y) =\n",
    "\\begin{bmatrix}\n",
    "0.5 & 6.5 \\\\\n",
    "-1.0 & -2.5\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4812f3f",
   "metadata": {},
   "source": [
    "## 5. Добавление ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7b279b",
   "metadata": {},
   "source": [
    "Функция активации ReLU определяется как\n",
    "\n",
    "$$\n",
    "\\text{ReLU}(x) = \\max(0, x).\n",
    "$$\n",
    "\n",
    "Хотя ReLU не является линейной операцией, она элемент-wise и не изменяет размерность выходного тензора.  \n",
    "Поэтому backend-системы могут включить её непосредственно в тот же вычислительный kernel, выполняя операцию активации сразу после свёртки без записи промежуточных данных в память.\n",
    "\n",
    "Результатом становится **fused-kernel** (например, `ConvBnRelu`), выполняющий все три операции за один проход."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0dc9c2",
   "metadata": {},
   "source": [
    "## 6. Оптимизация графа в ONNX Runtime и TensorRT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92193caa",
   "metadata": {},
   "source": [
    "Современные backend-системы выполняют оптимизацию графа вычислений в несколько этапов.\n",
    "\n",
    "1. **Поиск паттернов**  \n",
    "   Граф анализируется на наличие последовательностей узлов Conv → BatchNorm → ReLU.\n",
    "\n",
    "2. **Пересчёт параметров**  \n",
    "   Для каждой найденной последовательности вычисляются:\n",
    "   $$\n",
    "   a = \\frac{\\gamma}{\\sqrt{\\sigma^2 + \\epsilon}}, \\quad c = \\beta - a\\mu,\n",
    "   $$\n",
    "   $$\n",
    "   W' = aW, \\quad b' = ab + c.\n",
    "   $$\n",
    "\n",
    "3. **Переписывание графа**  \n",
    "   Узлы BatchNorm и ReLU удаляются, а Conv заменяется новой операцией с параметрами $ W', b' $.  \n",
    "   В некоторых системах (например, TensorRT) создаётся специальный узел `FusedConv`.\n",
    "\n",
    "4. **Преимущества**  \n",
    "   - Уменьшается количество обращений к памяти.  \n",
    "   - Сокращается число запусков GPU-ядер.  \n",
    "   - Снижается накладной overhead синхронизации.  \n",
    "   - Повышается общая пропускная способность вычислений."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ceb84ef",
   "metadata": {},
   "source": [
    "## 7. Паттерны оптимизации"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a276ba",
   "metadata": {},
   "source": [
    "ONNX и TensorRT поддерживают оптимизацию и объединение различных паттернов нейронных сетей, помимо классического сочетания convolution + batch normalization + ReLU (CBR). Некоторые из них включают:\n",
    "\n",
    "1. **Свёрточные слои с добавлением смещения (bias) и активацией**.\n",
    "\n",
    "2. **Полносвязные (линейные) слои с активацией**.\n",
    "\n",
    "3. **Операции пулинга (Pooling) с последующими слоями**.\n",
    "\n",
    "4. **Нормализация по группам (Group Normalization)**.\n",
    "\n",
    "5. **Операции квантования и деквантования (Q/DQ)**.\n",
    "\n",
    "6. **Трансформер-архитектуры (например, BERT, GPT)**.\n",
    "\n",
    "7. **Динамические формы входных данных**.\n",
    "\n",
    "8. **Операции с разреженными матрицами**.\n",
    "\n",
    "9. **Кастомные операции через плагины**.\n",
    "\n",
    "10. **Оптимизация для конкретных GPU**.\n",
    "\n",
    "Эти паттерны и оптимизации позволяют значительно ускорить инференс моделей на NVIDIA GPU, сокращая задержку и повышая пропускную способность."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b27dcb",
   "metadata": {},
   "source": [
    "## 7. Вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8b2946",
   "metadata": {},
   "source": [
    "Слияние Conv, BatchNorm и ReLU основано на линейности их математических преобразований.  \n",
    "BatchNorm при inference представляет собой линейную операцию, поэтому её можно объединить с параметрами свёртки:\n",
    "\n",
    "$$\n",
    "W' = aW, \\quad b' = ab + (\\beta - a\\mu).\n",
    "$$\n",
    "\n",
    "Это позволяет сократить вычислительный граф без потери точности результатов.  \n",
    "В оптимизированном графе фреймворк выполняет эквивалентную последовательность операций значительно быстрее за счёт уменьшения обращений к памяти и объединения вычислений в единый fused kernel.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
