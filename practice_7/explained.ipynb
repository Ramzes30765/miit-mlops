{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcff6bbf",
   "metadata": {},
   "source": [
    "# Оптимизация графа вычислений: слияние Conv–BatchNorm–ReLU\n",
    "\n",
    "## 1. Введение\n",
    "\n",
    "Во время оптимизации графа вычислений фреймворки, такие как ONNX Runtime, TensorRT или OpenVINO, объединяют последовательные операции свёртки (Conv), нормализации (BatchNorm) и активации (ReLU) в одну эквивалентную, но более эффективную операцию.  \n",
    "\n",
    "Основная причина этого — уменьшение количества обращений к памяти и запусков отдельных вычислительных ядер на GPU или CPU.  \n",
    "Главное математическое основание: и свёртка, и нормализация представляют собой **линейные преобразования**, которые могут быть объединены в одно эквивалентное линейное преобразование.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Свёрточная операция (Convolution)\n",
    "\n",
    "Свёртка в свёрточной нейронной сети — это операция линейного преобразования, вычисляемая по формуле\n",
    "\n",
    "$$\n",
    "Y[i,j] = \\sum_{u=0}^{H_W-1} \\sum_{v=0}^{W_W-1} W[u,v] \\cdot X[i+u, j+v] + b\n",
    "$$\n",
    "\n",
    "где  \n",
    "- \\( X \\) — входное изображение,  \n",
    "- \\( W \\) — фильтр (ядро свёртки),  \n",
    "- \\( b \\) — смещение (bias),  \n",
    "- \\( Y \\) — выход свёртки,  \n",
    "- \\( H_W, W_W \\) — высота и ширина фильтра.\n",
    "\n",
    "### 2.1. Параметры свёртки\n",
    "\n",
    "- **Stride (шаг свёртки)** — насколько ядро сдвигается по входу.  \n",
    "- **Padding** — добавление рамки нулей вокруг входа.  \n",
    "- **Valid padding** — свёртка без добавления нулей (padding = 0).  \n",
    "- **Same padding** — паддинг подбирается так, чтобы выход имел те же размеры, что и вход.\n",
    "\n",
    "Для valid-свёртки (padding = 0) размер выхода равен:\n",
    "\n",
    "$$\n",
    "H_{out} = H_{in} - H_W + 1, \\quad\n",
    "W_{out} = W_{in} - W_W + 1\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Batch Normalization\n",
    "\n",
    "Batch Normalization в режиме обучения нормализует активации внутри батча, а затем масштабирует и сдвигает их с помощью обучаемых параметров \\( \\gamma \\) и \\( \\beta \\):\n",
    "\n",
    "$$\n",
    "\\text{BN}(y) = \\gamma \\frac{y - \\mu_{\\text{batch}}}{\\sqrt{\\sigma^2_{\\text{batch}} + \\epsilon}} + \\beta\n",
    "$$\n",
    "\n",
    "- \\( \\mu_{\\text{batch}} \\) — среднее по батчу,  \n",
    "- \\( \\sigma^2_{\\text{batch}} \\) — дисперсия по батчу,  \n",
    "- \\( \\epsilon \\) — стабилизирующая константа,  \n",
    "- \\( \\gamma, \\beta \\) — обучаемые параметры.\n",
    "\n",
    "### 3.1. BatchNorm на inference\n",
    "\n",
    "На этапе inference используются накопленные статистики (running mean и running variance):\n",
    "\n",
    "$$\n",
    "\\text{BN}(y) = \\gamma \\frac{y - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta\n",
    "$$\n",
    "\n",
    "Это линейная операция над каждым элементом \\( y \\), которую можно записать в виде\n",
    "\n",
    "$$\n",
    "\\text{BN}(y) = a y + c,\n",
    "$$\n",
    "\n",
    "где\n",
    "\n",
    "$$\n",
    "a = \\frac{\\gamma}{\\sqrt{\\sigma^2 + \\epsilon}}, \\quad c = \\beta - a \\mu.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Слияние Conv и BatchNorm\n",
    "\n",
    "Пусть свёртка вычисляет\n",
    "\n",
    "$$\n",
    "Y = W * X + b\n",
    "$$\n",
    "\n",
    "а BatchNorm применяет\n",
    "\n",
    "$$\n",
    "\\text{BN}(Y) = aY + c.\n",
    "$$\n",
    "\n",
    "Подставим выражение для \\( Y \\) в формулу BatchNorm:\n",
    "\n",
    "$$\n",
    "\\text{BN}(W * X + b) = a(W * X + b) + c = (aW) * X + (ab + c).\n",
    "$$\n",
    "\n",
    "Таким образом, можно определить новые параметры свёртки:\n",
    "\n",
    "$$\n",
    "W' = aW, \\quad b' = ab + c.\n",
    "$$\n",
    "\n",
    "Теперь новая свёртка\n",
    "\n",
    "$$\n",
    "Y' = W' * X + b'\n",
    "$$\n",
    "\n",
    "эквивалентна последовательному применению Conv и BatchNorm.\n",
    "\n",
    "---\n",
    "\n",
    "### 4.1. Числовой пример\n",
    "\n",
    "Рассмотрим минимальный случай:  \n",
    "вход \\( X \\in \\mathbb{R}^{3 \\times 3} \\), ядро \\( W \\in \\mathbb{R}^{2 \\times 2} \\), bias \\( b = 0 \\).\n",
    "\n",
    "$$\n",
    "X =\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 0\\\\\n",
    "0 & 1 & 3\\\\\n",
    "2 & 2 & 1\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "W =\n",
    "\\begin{bmatrix}\n",
    "1 & 0\\\\\n",
    "-1 & 2\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Вычисляем valid-свёртку (stride=1, padding=0):\n",
    "\n",
    "$$\n",
    "Y =\n",
    "\\begin{bmatrix}\n",
    "3 & 7\\\\\n",
    "2 & 1\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Теперь применим BatchNorm с параметрами  \n",
    "\\( \\gamma = 3, \\beta = -1, \\mu = 2, \\sigma^2 = 4, \\epsilon = 0 \\).\n",
    "\n",
    "Сначала вычислим коэффициенты:\n",
    "\n",
    "$$\n",
    "a = \\frac{3}{\\sqrt{4}} = 1.5, \\quad c = \\beta - a\\mu = -1 - 1.5\\cdot2 = -4.\n",
    "$$\n",
    "\n",
    "BatchNorm выполняет:\n",
    "\n",
    "$$\n",
    "\\text{BN}(Y) = 1.5Y - 4 =\n",
    "\\begin{bmatrix}\n",
    "0.5 & 6.5\\\\\n",
    "-1 & -2.5\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Теперь пересчитаем параметры свёртки:\n",
    "\n",
    "$$\n",
    "W' = 1.5W =\n",
    "\\begin{bmatrix}\n",
    "1.5 & 0\\\\\n",
    "-1.5 & 3\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "b' = 1.5\\cdot0 - 4 = -4.\n",
    "$$\n",
    "\n",
    "Новая свёртка:\n",
    "\n",
    "$$\n",
    "Y' = W' * X + b'\n",
    "$$\n",
    "\n",
    "даёт тот же результат:\n",
    "\n",
    "$$\n",
    "Y' =\n",
    "\\begin{bmatrix}\n",
    "0.5 & 6.5\\\\\n",
    "-1 & -2.5\n",
    "\\end{bmatrix}\n",
    "= \\text{BN}(Y).\n",
    "$$\n",
    "\n",
    "Таким образом, операции Conv и BatchNorm полностью эквивалентны одной свёртке с параметрами \\( W', b' \\).\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Добавление ReLU\n",
    "\n",
    "Функция активации ReLU определяется как\n",
    "\n",
    "$$\n",
    "\\text{ReLU}(x) = \\max(0, x).\n",
    "$$\n",
    "\n",
    "Хотя ReLU не является линейной операцией, она элемент-wise и не изменяет размерность выходного тензора.  \n",
    "Поэтому backend-системы могут включить её непосредственно в тот же вычислительный kernel, выполняя операцию активации сразу после свёртки без записи промежуточных данных в память.\n",
    "\n",
    "Результатом становится **fused-kernel** (например, `ConvBnRelu`), выполняющий все три операции за один проход.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Оптимизация графа в ONNX Runtime и TensorRT\n",
    "\n",
    "Современные backend-системы выполняют оптимизацию графа вычислений в несколько этапов.\n",
    "\n",
    "1. **Поиск паттернов**  \n",
    "   Граф анализируется на наличие последовательностей узлов Conv → BatchNorm → ReLU.\n",
    "\n",
    "2. **Пересчёт параметров**  \n",
    "   Для каждой найденной последовательности вычисляются:\n",
    "   $$\n",
    "   a = \\frac{\\gamma}{\\sqrt{\\sigma^2 + \\epsilon}}, \\quad c = \\beta - a\\mu,\n",
    "   $$\n",
    "   $$\n",
    "   W' = aW, \\quad b' = ab + c.\n",
    "   $$\n",
    "\n",
    "3. **Переписывание графа**  \n",
    "   Узлы BatchNorm и ReLU удаляются, а Conv заменяется новой операцией с параметрами \\( W', b' \\).  \n",
    "   В некоторых системах (например, TensorRT) создаётся специальный узел `FusedConv`.\n",
    "\n",
    "4. **Преимущества**  \n",
    "   - Уменьшается количество обращений к памяти.  \n",
    "   - Сокращается число запусков GPU-ядер.  \n",
    "   - Снижается накладной overhead синхронизации.  \n",
    "   - Повышается общая пропускная способность вычислений.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Вывод\n",
    "\n",
    "Слияние Conv, BatchNorm и ReLU основано на линейности их математических преобразований.  \n",
    "BatchNorm при inference представляет собой линейную операцию, поэтому её можно объединить с параметрами свёртки:\n",
    "\n",
    "$$\n",
    "W' = aW, \\quad b' = ab + (\\beta - a\\mu).\n",
    "$$\n",
    "\n",
    "Это позволяет сократить вычислительный граф без потери точности результатов.  \n",
    "В оптимизированном графе фреймворк выполняет эквивалентную последовательность операций значительно быстрее за счёт уменьшения обращений к памяти и объединения вычислений в единый fused kernel.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
